{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask import array\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import tqdm\n",
    "import datetime\n",
    "import time\n",
    "import dill\n",
    "from scipy import sparse\n",
    "import os.path as path\n",
    "from os import mkdir\n",
    "\n",
    "from data import  Graph\n",
    "node_count = 63731\n",
    "\n",
    "# get node list by degree ranks\n",
    "with open('degree.txt', 'r') as f:\n",
    "    degree_data = f.read()\n",
    "\n",
    "degree_data = degree_data.split('\\n')\n",
    "degree_list = [int(a) for a in degree_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 10000\n",
    "degree_list = degree_list[-limit:]\n",
    "node_check = {}\n",
    "for a in degree_list:\n",
    "    node_check[a] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 13195.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading facebook links data\n",
      "convert data to dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# network modeling\n",
    "# read graph data\n",
    "link_columns = ['user_1', 'user_2']\n",
    "# read facebook links data then remove unnecessary nodes\n",
    "print('reading facebook links data')\n",
    "# file = gzip.GzipFile('facebook-links.txt.gz', 'rb')\n",
    "file = open('tmp_graph.txt', 'r')\n",
    "data = file.read()\n",
    "text_data = data#.decode('ascii')\n",
    "links_data_arr = []\n",
    "lines = text_data.split('\\n')\n",
    "for line in tqdm.tqdm(lines):\n",
    "    try:\n",
    "        splits = line.split('\\t')\n",
    "        user_1 = int(splits[0]) - 1\n",
    "        user_2 = int(splits[1]) - 1\n",
    "#         if node_check.get(user_1, None) is None or node_check.get(user_2, None) is None:\n",
    "#             continue\n",
    "        if user_1 > user_2:\n",
    "            user_1 += user_2\n",
    "            user_2 = user_1 - user_2\n",
    "            user_1 = user_1 - user_2\n",
    "        links_data_arr.append([user_1, user_2])\n",
    "    except:\n",
    "        continue\n",
    "file.close()\n",
    "print('convert data to dataframe')\n",
    "links_data_arr = np.array(links_data_arr)\n",
    "link_data_df = pd.DataFrame(links_data_arr, columns=link_columns)\n",
    "link_data_df = link_data_df.drop_duplicates() # remove duplicated links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_1</th>\n",
       "      <th>user_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_1  user_2\n",
       "0       0       3\n",
       "1       0       4\n",
       "2       1       2\n",
       "3       1       4\n",
       "4       2       4\n",
       "5       3       4\n",
       "6       2       5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_subgraph_new(graph, pe=0.5):\n",
    "    g = graph.copy()\n",
    "    g = shuffle(g, random_state=int(time.time() * 100000 % 100000)).reset_index()[['user_1', 'user_2']]\n",
    "    size = int(g.shape[0] * pe)\n",
    "    print('size',size)\n",
    "    start = random.randint(0, g.shape[0] - size)\n",
    "    g = g.iloc[start:start + size]\n",
    "    node_values = list(set(g.values.flatten()))\n",
    "    node_alias = {}\n",
    "    count = 0\n",
    "    for val in node_values:\n",
    "        node_alias[val] = count\n",
    "        count += 1\n",
    "    return g, node_alias, node_values\n",
    "\n",
    "def gen_subgraph_new_2(graph, pe=0.5):\n",
    "    g1 = shuffle(graph.copy(),random_state=int(time.time()))\n",
    "    degree = graph.groupby('user_1')['user_1'].count().to_frame('count').reset_index().append(\n",
    "graph.groupby('user_2')['user_2'].count().to_frame('count').reset_index().rename(columns={'user_2':'user_1'}).reset_index()).groupby('user_1').sum()['count'].to_dict()\n",
    "\n",
    "    g1 = g1.reset_index().assign(drop=pd.Series(np.zeros(g1.shape[0])))\n",
    "    count = 0\n",
    "    drop_dict = g1['drop'].to_dict()\n",
    "    t1 = datetime.datetime.now()\n",
    "    for i in range(0, g1.shape[0]):\n",
    "        row = g1.iloc[i]\n",
    "        user_1 = row['user_1']\n",
    "        user_2 = row['user_2']\n",
    "        # if degree[user_1] >= 2 and degree[user_2] >= 2 and drop_dict[i] != 1:\n",
    "        if random.random() >pe:\n",
    "            drop_dict[i] = 1\n",
    "            degree[user_1] -= 1\n",
    "            degree[user_2] -= 1\n",
    "            count += 1\n",
    "            if count % 10000 == 0:\n",
    "                print(count, datetime.datetime.now() - t1)\n",
    "                t1 = datetime.datetime.now()\n",
    "            # if count >= g1_size:\n",
    "            #     break\n",
    "    g1 = g1[~pd.Series(drop_dict).astype(bool)][['user_1', 'user_2']]\n",
    "    node_values = list(set(g1.values.flatten()))\n",
    "    node_alias = {}\n",
    "    count = 0\n",
    "    for val in node_values:\n",
    "        node_alias[val] = count\n",
    "        count += 1\n",
    "    return g1, node_alias, node_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size 7\n"
     ]
    }
   ],
   "source": [
    "pe = 1\n",
    "g, node_alias, node_values = gen_subgraph_new(link_data_df, pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(node_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 2968.07it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 23541.46it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 27010.24it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 42013.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process matrix (7, 3)\n",
      "get neighbors for every node\n",
      "calculate graph weights\n",
      "0:00:00.004803\n",
      "calculate sums\n",
      "0\n",
      "calculate forward and jump matrix\n",
      "forward matrix\n",
      "jump matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 6/6 [00:00<00:00, 2598.70it/s]\n"
     ]
    }
   ],
   "source": [
    "g['weights'] = g['user_1'].copy()\n",
    "print('process matrix', g.shape)\n",
    "\n",
    "node_count = len(node_alias)\n",
    "# data_matrix = np.zeros((node_count, node_count), dtype='float')\n",
    "# edge_matrix = np.zeros((node_count, node_count), dtype=np.uint8)\n",
    "data_matrix = np.array([[0, 0, 0, 0.67, 0.4, 0],\n",
    "                        [0, 0, 0.5, 0, 0.4, 0],\n",
    "                        [0, 0.5, 0, 0, 0.33, 0.25],\n",
    "                        [0.67, 0, 0, 0, 0.4, 0],\n",
    "                        [0.4, 0.4, 0.33, 0.4, 0, 0],\n",
    "                        [0, 0, 0.25, 0, 0, 0]])\n",
    "edge_matrix = (data_matrix > 0).astype(int)\n",
    "t1 = datetime.datetime.now()\n",
    "g1_values = g.values.copy().astype(float)\n",
    "print('get neighbors for every node')\n",
    "neighbors_map = {}\n",
    "for i in tqdm.trange(g1_values.shape[0]):\n",
    "    row = g.iloc[i]\n",
    "    ## neighbors for user_1\n",
    "    user_1 = node_alias[row.user_1]\n",
    "    user_2 = node_alias[row.user_2]\n",
    "    user_1_neighbors = neighbors_map.get(user_1, set())\n",
    "    user_1_neighbors.add(user_2)\n",
    "    neighbors_map[user_1] = user_1_neighbors\n",
    "    ## neighbors for user_2\n",
    "    user_2_neighbors = neighbors_map.get(user_2, set())\n",
    "    user_2_neighbors.add(user_1)\n",
    "    neighbors_map[user_2] = user_2_neighbors\n",
    "print('calculate graph weights')\n",
    "for i in range(g1_values.shape[0]):\n",
    "    row = g1_values[i]\n",
    "    user_1 = node_alias[int(row[0])]\n",
    "    user_2 = node_alias[int(row[1])]\n",
    "    user_1_neighbors = neighbors_map[user_1]\n",
    "    user_2_neighbors = neighbors_map[user_2]\n",
    "    # w = (1 + len(user_1_neighbors.intersection(user_2_neighbors)))/(1 + len(user_1_neighbors.union(user_2_neighbors)))\n",
    "    w = (len(user_1_neighbors.intersection(user_2_neighbors)))/(len(user_1_neighbors.union(user_2_neighbors)))\n",
    "    row[2] = w\n",
    "    # row[2] = 1\n",
    "\n",
    "# for i in range(g1_values.shape[0]):\n",
    "#     row = g1_values[i]\n",
    "#     row[0] = alias[row[0]]\n",
    "#     row[1] = alias[row[1]]\n",
    "\n",
    "for row in g1_values:\n",
    "    user_1 = node_alias[int(row[0])]\n",
    "    user_2 = node_alias[int(row[1])]\n",
    "#     data_matrix[user_1,user_2] = row[2]\n",
    "#     data_matrix[user_2,user_1] = row[2]\n",
    "    edge_matrix[user_1,user_2] = 1\n",
    "    edge_matrix[user_2,user_1] = 1\n",
    "\n",
    "print(datetime.datetime.now() - t1)\n",
    "print('calculate sums')\n",
    "total_weight_sum = data_matrix.sum()\n",
    "neighbor_count = data_matrix.sum(axis=1)\n",
    "\n",
    "total_neighbor_strengs = np.array([data_matrix[i][edge_matrix[i] > 0].sum() for i in tqdm.trange(node_count)])\n",
    "print((total_neighbor_strengs == 0).astype(int).sum())\n",
    "print('calculate forward and jump matrix')    \n",
    "\n",
    "print('forward matrix')\n",
    "## old way\n",
    "forward_index_1 = []\n",
    "forward_index_2 = []\n",
    "forward_data = []\n",
    "forward_bin = []\n",
    "jump_index_1=[]\n",
    "jump_index_2=[]\n",
    "jump_data = []\n",
    "for k in tqdm.trange(g1_values.shape[0]):\n",
    "    row = g1_values[k]\n",
    "    i = node_alias[int(row[0])]\n",
    "    j = node_alias[int(row[1])]\n",
    "    forward_index_1.append(i)\n",
    "    forward_index_2.append(j)\n",
    "    # if total_neighbor_strengs[i] != 0:\n",
    "    #     forward_data.append(neighbor_count[j] * 1.0 / total_neighbor_strengs[i])\n",
    "    if neighbor_count[i] > 0:\n",
    "        forward_data.append(data_matrix[i, j]/ neighbor_count[i])\n",
    "    else: forward_data.append(0)\n",
    "\n",
    "    forward_index_1.append(j)\n",
    "    forward_index_2.append(i)\n",
    "    # if total_neighbor_strengs[j] != 0:\n",
    "    #     forward_data.append(neighbor_count[i] * 1.0 / total_neighbor_strengs[j])\n",
    "    if neighbor_count[j] > 0:\n",
    "        forward_data.append(data_matrix[i, j]/ neighbor_count[j])\n",
    "    else: forward_data.append(0)\n",
    "\n",
    "for k in tqdm.trange(node_count):\n",
    "    forward_index_1.append(k)\n",
    "    forward_index_2.append(k)\n",
    "    forward_data.append(0)\n",
    "    # if total_neighbor_strengs[k] > 0:\n",
    "    #     # u = node_values[k]\n",
    "    #     u = k\n",
    "    #     forward_index_1.append(u)\n",
    "    #     forward_index_2.append(u)\n",
    "    #     forward_data.append(neighbor_count[u] * 1.0 / total_neighbor_strengs[u])\n",
    "forward_matrix = sparse.csc_matrix((forward_data, (forward_index_1, forward_index_2)))\n",
    "forward_data_file = 'forward_data_file.tmp'  \n",
    "forward_data_memmap = np.memmap(forward_data_file, dtype='float32', mode='w+', shape=(node_count, node_count))\n",
    "forward_data_memmap[:] = forward_matrix.toarray()[:]\n",
    "forward_data_memmap.flush()\n",
    "\n",
    "print('jump matrix')\n",
    "jump_data_file = 'jump_data_file.tmp'\n",
    "jump_data_memmap = np.memmap(jump_data_file, dtype='float32', mode='w+', shape=(node_count, node_count))\n",
    "tmp = np.array(total_neighbor_strengs).sum()\n",
    "for  i in tqdm.trange(0, node_count):\n",
    "    jump_data_memmap[i] = neighbor_count/tmp\n",
    "jump_data_memmap.flush()\n",
    "\n",
    "jump_data_file = 'jump_data_file.tmp'\n",
    "jump_data_memmap = np.memmap(jump_data_file, dtype='float32', mode='r+', shape=(node_count, node_count))\n",
    "forward_data_file = 'forward_data_file.tmp'  \n",
    "forward_data_memmap = np.memmap(forward_data_file, dtype='float32', mode='r+', shape=(node_count, node_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 5256.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating T matrix\n",
      "1 [0.20766684 0.21314681 0.16445723 0.20766684 0.17680029 0.18433325] 0:00:00.012433 0.35630198429117665\n",
      "new\n",
      "2 [0.19544878 0.17319161 0.19480549 0.19544878 0.19855286 0.16852867] 0:00:00.011720 0.1322967255271414\n",
      "new\n",
      "3 [0.19585783 0.19574037 0.1816223  0.19585783 0.18980752 0.19432469] 0:00:00.010521 0.07109140606730283\n",
      "new\n",
      "4 [0.19329665 0.18621118 0.19329991 0.19329665 0.19258324 0.18311899] 0:00:00.012060 0.04031056645717929\n",
      "new\n",
      "5 [0.19281549 0.19277422 0.18806609 0.19281549 0.19146824 0.19304495] 0:00:00.011472 0.023800140791142538\n",
      "new\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [0.1922051  0.18988147 0.1923122  0.1922051  0.19175331 0.18859621] 0:00:00.011658 0.013093459328456764\n",
      "new\n",
      "7 [0.19197081 0.19199427 0.19037256 0.19197081 0.19161765 0.19220541] 0:00:00.006560 0.008265888823543788\n",
      "new\n",
      "8 [0.191803   0.19102708 0.19187889 0.191803   0.19162743 0.19055671] 0:00:00.005818 0.004467617298984761\n",
      "new\n",
      "9 [0.1917168  0.19174211 0.19117643 0.1917168  0.19161408 0.1918371 ] 0:00:00.007083 0.0028836289706672402\n",
      "new\n",
      "10 [0.19166668 0.19140535 0.19170627 0.19166668 0.19160588 0.19124001] 0:00:00.006172 0.001572132560504702\n",
      "new\n",
      "11 [0.19163739 0.19165245 0.19145414 0.19163739 0.19160591 0.19169037] 0:00:00.003591 0.0010081901111657632\n",
      "new\n",
      "12 [0.19162182 0.1915334  0.19164    0.19162182 0.19160158 0.19147606] 0:00:00.003634 0.0005546992086892244\n",
      "new\n",
      "13 [0.19161216 0.19161953 0.19154986 0.19161216 0.19160228 0.19163405] 0:00:00.002734 0.0003542808351440774\n",
      "new\n",
      "14 [0.19160724 0.19157723 0.19161502 0.19160724 0.1916006  0.19155743] 0:00:00.003260 0.00019559594280546544\n",
      "new\n",
      "15 [0.19160409 0.19160737 0.19158287 0.19160409 0.19160096 0.19161282] 0:00:00.003209 0.00012434621064760787\n",
      "new\n",
      "16 [0.19160253 0.19159233 0.19160572 0.19160253 0.19160037 0.19158549] 0:00:00.002922 6.894674217361452e-05\n",
      "consumed time: 0:00:00.133094\n",
      "save graph data\n"
     ]
    }
   ],
   "source": [
    "# calculate T matrix\n",
    "print('calculating T matrix')\n",
    "T_file = 'T.tmp'\n",
    "pf = 0.85\n",
    "pj = 0.15\n",
    "T = np.memmap(T_file, dtype='float32', mode='w+', shape=(node_count, node_count))\n",
    "for i in tqdm.trange(node_count):\n",
    "    T[i] = jump_data_memmap[i] * pj + forward_data_memmap[i] * pf\n",
    "## write to memmap file\n",
    "T.flush()\n",
    "jump_data_memmap.flush()\n",
    "forward_data_memmap.flush()\n",
    "del jump_data_memmap\n",
    "del forward_data_memmap\n",
    "\n",
    "# GlobalRank computation\n",
    "t1 = datetime.datetime.now()\n",
    "R = np.array(neighbor_count)/total_weight_sum #* factor#/total_weight_sum\n",
    "dill.dump(R, open('R.pickle', 'wb'))\n",
    "R_dask = dask.array.from_array(R, chunks=500) # old = 1650\n",
    "threshold = 0.0001\n",
    "count = 0\n",
    "T_dask = dask.array.from_array(T, chunks=500)\n",
    "while True:\n",
    "    t = datetime.datetime.now()\n",
    "    count += 1\n",
    "    R_dask = dask.array.from_array(R, chunks=500)\n",
    "    R_dask_tmp = dask.array.dot(T_dask, R_dask)\n",
    "    R_tmp = R_dask_tmp.compute()\n",
    "    delta = R_tmp - R\n",
    "#     sigma = (delta**2).sum()#.compute()\n",
    "    sigma = np.abs(delta).sum()\n",
    "    print(count, R_tmp, datetime.datetime.now() - t, sigma)\n",
    "    R = R_tmp\n",
    "    if sigma <= threshold:\n",
    "        break\n",
    "    print('new')\n",
    "print('consumed time:', datetime.datetime.now() - t1)\n",
    "print('save graph data')\n",
    "# dill.dump(R, open('R_{}.pickle'.format(idx), 'wb'))\n",
    "##convert neighbors_map to dict<int, list>\n",
    "for key in neighbors_map.keys():\n",
    "    val = neighbors_map[key]\n",
    "    val = list(val)\n",
    "    neighbors_map[key] = val\n",
    "g_aliased_values = []\n",
    "for row in g1_values:\n",
    "    u_1 = node_alias[row[0]]\n",
    "    u_2 = node_alias[row[1]]\n",
    "    g_aliased_values.append([u_1, u_2])\n",
    "g_new = pd.DataFrame(np.array(g_aliased_values), columns=['user_1', 'user_2'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
